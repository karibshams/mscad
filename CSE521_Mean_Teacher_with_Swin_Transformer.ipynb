{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12571866,"sourceType":"datasetVersion","datasetId":7939307}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Mean Teacher + Swin**","metadata":{}},{"cell_type":"markdown","source":"**1. Import**","metadata":{}},{"cell_type":"code","source":"# Import\n\nimport math, random, time, copy, os, json, warnings, pathlib\nfrom pathlib import Path\nimport numpy as np\nfrom tqdm.auto import tqdm\nimport matplotlib.pyplot as plt\n\nimport torch, torch.nn as nn, torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset, Subset\nfrom torchvision import transforms, datasets, models\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.metrics import precision_recall_fscore_support, accuracy_score\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nfrom torch.cuda.amp import autocast, GradScaler\n\nimport timm  # ADDED: for Swin Transformer\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nSEED = 42\ntorch.manual_seed(SEED); np.random.seed(SEED); random.seed(SEED)\ntorch.cuda.manual_seed(42)\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T06:44:34.699967Z","iopub.execute_input":"2025-07-25T06:44:34.700245Z","iopub.status.idle":"2025-07-25T06:44:34.708024Z","shell.execute_reply.started":"2025-07-25T06:44:34.700224Z","shell.execute_reply":"2025-07-25T06:44:34.707333Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Device Check**","metadata":{}},{"cell_type":"code","source":"# Device\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(\"Using device:\", device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T06:44:34.709115Z","iopub.execute_input":"2025-07-25T06:44:34.709337Z","iopub.status.idle":"2025-07-25T06:44:34.720114Z","shell.execute_reply.started":"2025-07-25T06:44:34.709321Z","shell.execute_reply":"2025-07-25T06:44:34.719504Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**2. Dataset Load**","metadata":{}},{"cell_type":"code","source":"# Dataset Load\n\ndata_dir = \"/kaggle/input/kidney-dataset/CT-KIDNEY-DATASET-Normal-Cyst-Tumor-Stone/CT-KIDNEY-DATASET-Normal-Cyst-Tumor-Stone\"\n\nbase_tf = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5,), (0.5,))\n])\n\n\nweak_tf   = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomCrop(224, padding=4),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5,), (0.5,))\n])\nstrong_tf = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.RandomGrayscale(p=0.2),\n    transforms.GaussianBlur(kernel_size=3, sigma=(0.3, 2.0)),\n    transforms.RandAugment(7, 15),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406],\n                         [0.229, 0.224, 0.225])\n])\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T06:44:34.720881Z","iopub.execute_input":"2025-07-25T06:44:34.721191Z","iopub.status.idle":"2025-07-25T06:44:34.733876Z","shell.execute_reply.started":"2025-07-25T06:44:34.721167Z","shell.execute_reply":"2025-07-25T06:44:34.733146Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**3. custom dataset to return weak and strong augmentations**","metadata":{}},{"cell_type":"code","source":"# dataset (weak + strong augmentations) \n\nclass UnlabeledPair(Dataset):\n    def __init__(self, base_dataset, weak_tf, strong_tf):\n        self.base_dataset = base_dataset\n        self.weak_tf = weak_tf\n        self.strong_tf = strong_tf\n\n    def __getitem__(self, idx):\n        img, _ = self.base_dataset[idx]\n        if isinstance(img, torch.Tensor):\n            img = transforms.ToPILImage()(img)\n        return self.weak_tf(img), self.strong_tf(img)\n\n    def __len__(self):\n        return len(self.base_dataset)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T06:44:34.735309Z","iopub.execute_input":"2025-07-25T06:44:34.735539Z","iopub.status.idle":"2025-07-25T06:44:34.748339Z","shell.execute_reply.started":"2025-07-25T06:44:34.735525Z","shell.execute_reply":"2025-07-25T06:44:34.747639Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**4. Split Dataset**","metadata":{}},{"cell_type":"code","source":"# Build splits: (train_labeled, train_unlabeled, val)\n\nFULL = datasets.ImageFolder(data_dir)\n\n# ----  extract labels for stratified splitting   ------\n\nlabels = [lbl for _, lbl in FULL.samples]\nlabels = np.array(labels)\n\n# --- split into training and validation sets (90% train, 10% val) ---\nsss = StratifiedShuffleSplit(n_splits=1, test_size=0.10, random_state=SEED)\ntrain_idx, val_idx = next(sss.split(np.zeros(len(labels)), labels))\ntrain_labels = labels[train_idx]\n\n# --- split training set into labeled and unlabeled (50% labeled) ---\np_labeled = 0.50\nsss2 = StratifiedShuffleSplit(n_splits=1, test_size=1 - p_labeled,\n                              random_state=SEED)\nlab_idx, unlab_idx = next(sss2.split(np.zeros(len(train_labels)), train_labels))\n\nlab_idx   = train_idx[lab_idx]\nunlab_idx = train_idx[unlab_idx]\n\n# ---------- labelled ----------\ntrain_lab_ds = Subset(\n    datasets.ImageFolder(data_dir, transform=weak_tf),\n    lab_idx)\n\n# ---------- unlabeled ----------\nbase_unlab   = Subset(\n    datasets.ImageFolder(data_dir),        # â† NO transform\n    unlab_idx)\n\n# wrap with UnlabeledPair to apply weak and strong augmentations\n\ntrain_unlab_ds = UnlabeledPair(base_unlab, weak_tf, strong_tf)\n\n# ---------- validation ----------\nval_ds = Subset(\n    datasets.ImageFolder(data_dir, transform=base_tf),\n    val_idx)\n\n# ---------- create data loaders ----------\nBATCH_L = 32\nBATCH_U = 64\nlab_loader   = DataLoader(train_lab_ds,   batch_size=BATCH_L,\n                          shuffle=True,  drop_last=True,  num_workers=2)\nunlab_loader = DataLoader(train_unlab_ds, batch_size=BATCH_U,\n                          shuffle=True,  drop_last=True,  num_workers=2)\nval_loader   = DataLoader(val_ds,         batch_size=64,\n                          shuffle=True, num_workers=2)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T06:44:34.749124Z","iopub.execute_input":"2025-07-25T06:44:34.749879Z","iopub.status.idle":"2025-07-25T06:44:38.798297Z","shell.execute_reply.started":"2025-07-25T06:44:34.749854Z","shell.execute_reply":"2025-07-25T06:44:38.797124Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**All the Swin transformer Model**","metadata":{}},{"cell_type":"code","source":"import timm\nmodels = timm.list_models('*swin*')\nprint(models)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T06:44:38.799233Z","iopub.execute_input":"2025-07-25T06:44:38.799525Z","iopub.status.idle":"2025-07-25T06:44:38.80791Z","shell.execute_reply.started":"2025-07-25T06:44:38.799496Z","shell.execute_reply":"2025-07-25T06:44:38.806086Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**5. Swin Backbone (tiny variant)**","metadata":{}},{"cell_type":"code","source":"# Define a custom backbone model using Swin Transformer (tiny variant)\n\n# === Swin Backbone + Head ===\nclass SwinBackbone(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.backbone = timm.create_model('swin_tiny_patch4_window7_224', \n                                          pretrained=True, num_classes=0)\n        for param in self.backbone.parameters():\n            param.requires_grad = False\n\n    def forward(self, x):\n        return self.backbone(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T06:44:38.810595Z","iopub.execute_input":"2025-07-25T06:44:38.810871Z","iopub.status.idle":"2025-07-25T06:44:38.822959Z","shell.execute_reply.started":"2025-07-25T06:44:38.810846Z","shell.execute_reply":"2025-07-25T06:44:38.822206Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**6. Linear Head**","metadata":{}},{"cell_type":"code","source":"# Define a linear classification head\nclass LinearHead(nn.Module):\n    def __init__(self, in_features=768, num_classes=4):\n        super().__init__()\n        self.classifier = nn.Linear(in_features, num_classes)\n\n    def forward(self, x):\n        return self.classifier(x)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T06:44:38.825621Z","iopub.execute_input":"2025-07-25T06:44:38.825887Z","iopub.status.idle":"2025-07-25T06:44:38.839296Z","shell.execute_reply.started":"2025-07-25T06:44:38.825866Z","shell.execute_reply":"2025-07-25T06:44:38.838226Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**7. Mean Teacher Model with Swin Transformer(Backbone)**","metadata":{}},{"cell_type":"code","source":"# Full model (Student/Teacher)\n\n# --- Swin Backbone + Linear Head wrapped in a model for Mean Teacher ----\nclass SwinMeanTeacherModel(nn.Module):\n    def __init__(self, num_classes=4):\n        super().__init__()\n        self.backbone = SwinBackbone()\n        self.head = LinearHead(768, num_classes)\n\n    def forward(self, x):\n        with torch.no_grad():\n            feats = self.backbone(x)\n        return self.head(feats)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T06:44:38.840271Z","iopub.execute_input":"2025-07-25T06:44:38.840521Z","iopub.status.idle":"2025-07-25T06:44:38.851726Z","shell.execute_reply.started":"2025-07-25T06:44:38.8405Z","shell.execute_reply":"2025-07-25T06:44:38.851046Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**8. Initialize Student and Teacher Models with EMA**","metadata":{}},{"cell_type":"code","source":"# Load the model\n\nnum_classes = 4  \nstudent = SwinMeanTeacherModel(num_classes).to(device)\nteacher = SwinMeanTeacherModel(num_classes).to(device)\n\n# Copy student weights to teacher initially\nteacher.load_state_dict(student.state_dict())  \n\nema_decay = 0.999\n\ndef update_ema(student, teacher, decay):\n    with torch.no_grad():\n        for s, t in zip(student.parameters(), teacher.parameters()):\n            t.data.mul_(decay).add_(s.data, alpha=1 - decay)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T06:44:38.852549Z","iopub.execute_input":"2025-07-25T06:44:38.852813Z","iopub.status.idle":"2025-07-25T06:44:39.877596Z","shell.execute_reply.started":"2025-07-25T06:44:38.852793Z","shell.execute_reply":"2025-07-25T06:44:39.877067Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**9. Sigmoid, Loss, Classification Report Define**","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import classification_report, accuracy_score, precision_recall_fscore_support\n\n# --- loss function and optimizer setup ---\ncriterion_sup = nn.CrossEntropyLoss()\nopt   = torch.optim.AdamW(student.parameters(), lr=3e-4, weight_decay=2e-4)\nsched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=100)\n\n# --- ramp-up function for consistency weight ---\ndef sigmoid_rampup(current, rampup_length):\n    if rampup_length == 0: return 1.0\n    current = np.clip(current, 0.0, rampup_length)\n    phase = 1.0 - current / rampup_length\n    return math.exp(-5.0 * phase * phase)\n    \ndef evaluate_model(model, loader):\n    model.eval()\n    all_p, all_t = [], []\n    with torch.no_grad():\n        for x, y in loader:\n            x, y = x.to(device), y.to(device)\n            logits = model(x)\n            preds = torch.argmax(logits, 1)\n            all_p.append(preds.cpu().numpy())\n            all_t.append(y.cpu().numpy())\n\n    y_pred = np.concatenate(all_p)\n    y_true = np.concatenate(all_t)\n\n    # Overall metrics\n    acc = accuracy_score(y_true, y_pred)\n    pr, rc, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro', zero_division=0)\n\n    return acc, pr, rc, f1\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T06:44:39.878306Z","iopub.execute_input":"2025-07-25T06:44:39.878486Z","iopub.status.idle":"2025-07-25T06:44:39.887026Z","shell.execute_reply.started":"2025-07-25T06:44:39.878463Z","shell.execute_reply":"2025-07-25T06:44:39.88632Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Class Name of the Dataset**","metadata":{}},{"cell_type":"code","source":"class_names = ['Cyst', 'Normal', 'Stone', 'Tumor']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T06:44:39.887993Z","iopub.execute_input":"2025-07-25T06:44:39.888705Z","iopub.status.idle":"2025-07-25T06:44:39.899381Z","shell.execute_reply.started":"2025-07-25T06:44:39.888687Z","shell.execute_reply":"2025-07-25T06:44:39.898635Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**10. Mean Teacher Training with Consistency Loss, Mixed Precision, and EMA**","metadata":{}},{"cell_type":"code","source":"best_f1, best_state = 0.0, None\nhistory = []\ntrain_loss_list = []\nval_loss_list = []\nEPOCHS = 200\nlambda_max = 30.0             \nramp_len  =   4  \n\n# gradient scaler for mixed precision\nscaler = GradScaler()\n\nfor epoch in range(1, EPOCHS+1):\n    student.train()\n    lab_iter, unlab_iter = iter(lab_loader), iter(unlab_loader)\n    n_steps = max(len(lab_loader), len(unlab_loader))\n    total_loss = 0\n\n    # --- training loop with progress bar ---\n    pbar = tqdm(range(n_steps), desc=f\"Epoch {epoch}/{EPOCHS}\", leave=False)\n    for _ in pbar:\n        try: \n            x_lab, y_lab = next(lab_iter)\n        except StopIteration: \n            lab_iter = iter(lab_loader); x_lab, y_lab = next(lab_iter)\n        try: \n            x_w, x_s = next(unlab_iter)\n        except StopIteration: \n            unlab_iter = iter(unlab_loader); x_w, x_s = next(unlab_iter)\n\n        x_lab, y_lab = x_lab.to(device), y_lab.to(device)\n        x_w, x_s = x_w.to(device), x_s.to(device)\n\n        opt.zero_grad()\n\n        # --- forward pass using autocast for mixed precision ---\n        with torch.amp.autocast(device_type='cuda'):\n            logits_lab = student(x_lab)\n            loss_sup   = criterion_sup(logits_lab, y_lab)\n\n            with torch.no_grad():\n                t_prob = F.softmax(teacher(x_w), dim=1)\n            s_prob = F.softmax(student(x_s), dim=1)\n            loss_cons = F.mse_loss(s_prob, t_prob)\n\n            lam = lambda_max * sigmoid_rampup(epoch-1, ramp_len)\n            loss = loss_sup + lam * loss_cons\n\n        # --- backward and optimizer step with scaler ---\n        scaler.scale(loss).backward()\n        scaler.step(opt)\n        scaler.update()\n\n        # --- update teacher model using EMA ---\n        update_ema(student, teacher, ema_decay)\n\n        total_loss += loss.item()\n        pbar.set_postfix(sup=loss_sup.item(), cons=loss_cons.item(), lam=lam)\n\n\n    # --- compute and store average training loss ---\n    avg_train_loss = total_loss / n_steps\n    train_loss_list.append(avg_train_loss)\n\n    # --- compute validation loss using teacher model ---\n    teacher.eval()\n    val_loss = 0\n    with torch.no_grad():\n        for x, y in val_loader:\n            x, y = x.to(device), y.to(device)\n            logits = teacher(x)\n            val_loss += criterion_sup(logits, y).item()\n    val_loss /= len(val_loader)\n    val_loss_list.append(val_loss)\n\n    acc, pr, rc, f1 = evaluate_model(teacher, val_loader)\n    history.append(dict(epoch=epoch, acc=acc, prec=pr, rec=rc, f1=f1))\n    print(f\"Epoch {epoch:02d}: train_loss={avg_train_loss:.4f} val_loss={val_loss:.4f} val_acc={acc:.4f} F1={f1:.4f}\")\n\n    if f1 > best_f1:\n        best_f1 = f1\n        best_state = copy.deepcopy(teacher.state_dict())\n        torch.save(best_state, \"best_mean_teacher.pth\")\n    \n\nprint(f\"\\nBest F1 = {best_f1:.4f}\")\nwith open(\"history.json\", \"w\") as f: json.dump(history, f, indent=2)\n\n# === Plot Loss Curves ===\nplt.figure(figsize=(8, 5))\nplt.plot(train_loss_list, label=\"Train Loss\")\nplt.plot(val_loss_list, label=\"Val Loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.title(\"Training and Validation Loss Curve\")\nplt.legend()\nplt.grid()\nplt.savefig(\"loss_curve.png\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T06:44:39.900125Z","iopub.execute_input":"2025-07-25T06:44:39.900341Z","iopub.status.idle":"2025-07-25T14:56:52.008171Z","shell.execute_reply.started":"2025-07-25T06:44:39.900317Z","shell.execute_reply":"2025-07-25T14:56:52.007125Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**11. Best Epoch Selection Based on F1-Score**","metadata":{}},{"cell_type":"code","source":"import json\n\nwith open(\"history.json\", \"r\") as f:\n    history = json.load(f)\n\n# Find the epoch with the highest F1-score\nbest_epoch_info = max(history, key=lambda x: x['f1'])\nprint(f\"Best epoch was: {best_epoch_info['epoch']} with F1 = {best_epoch_info['f1']:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T14:56:52.009854Z","iopub.execute_input":"2025-07-25T14:56:52.01017Z","iopub.status.idle":"2025-07-25T14:56:52.018789Z","shell.execute_reply.started":"2025-07-25T14:56:52.010135Z","shell.execute_reply":"2025-07-25T14:56:52.018193Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**12. Evaluation**","metadata":{}},{"cell_type":"code","source":"def evaluate_model(model, loader, class_names):\n    model.eval()\n    all_p, all_t = [], []\n    with torch.no_grad():\n        for x, y in loader:\n            x, y = x.to(device), y.to(device)\n            logits = model(x)\n            preds = torch.argmax(logits, 1)\n            all_p.append(preds.cpu().numpy())\n            all_t.append(y.cpu().numpy())\n\n    y_pred = np.concatenate(all_p)\n    y_true = np.concatenate(all_t)\n\n    # Overall metrics\n    acc = accuracy_score(y_true, y_pred)\n    pr, rc, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro', zero_division=0)\n\n    print(f\"\\nOverall Metrics:\")\n    print(f\"Accuracy : {acc:.4f}\")\n    print(f\"Precision: {pr:.4f}\")\n    print(f\"Recall   : {rc:.4f}\")\n    print(f\"F1 Score : {f1:.4f}\")\n\n    # Per-class report\n    print(f\"\\nPer-Class Metrics:\")\n    report = classification_report(y_true, y_pred, target_names=class_names, zero_division=0)\n    print(report)\n\n    return acc, pr, rc, f1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T14:56:52.019419Z","iopub.execute_input":"2025-07-25T14:56:52.019601Z","iopub.status.idle":"2025-07-25T14:56:52.033166Z","shell.execute_reply.started":"2025-07-25T14:56:52.019586Z","shell.execute_reply":"2025-07-25T14:56:52.032473Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**13. Classification Report**","metadata":{}},{"cell_type":"code","source":"class_names = ['Cyst', 'Normal', 'Stone', 'Tumor']\nevaluate_model(model=teacher, loader=val_loader, class_names=class_names)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T14:56:52.034018Z","iopub.execute_input":"2025-07-25T14:56:52.034228Z","iopub.status.idle":"2025-07-25T14:56:58.842239Z","shell.execute_reply.started":"2025-07-25T14:56:52.034213Z","shell.execute_reply":"2025-07-25T14:56:58.84137Z"}},"outputs":[],"execution_count":null}]}