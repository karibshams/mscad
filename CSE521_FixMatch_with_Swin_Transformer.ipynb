{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2764486,"sourceType":"datasetVersion","datasetId":1686903}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false},"colab":{"provenance":[]}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Import**","metadata":{}},{"cell_type":"code","source":"# 1. Imports\nimport os, random, math, itertools, numpy as np\nimport matplotlib.pyplot as plt\nimport timm\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch, torch.nn.functional as F\nfrom torch import nn, optim\nfrom torch.utils.data import Dataset, DataLoader, Subset, random_split\nfrom torchvision import datasets, transforms, models\nfrom tqdm.auto import tqdm, trange\nfrom sklearn.metrics import classification_report\n\nSEED = 42\nrandom.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Running on\", device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T20:18:16.829269Z","iopub.execute_input":"2025-07-22T20:18:16.829524Z","iopub.status.idle":"2025-07-22T20:18:24.815909Z","shell.execute_reply.started":"2025-07-22T20:18:16.8295Z","shell.execute_reply":"2025-07-22T20:18:24.815298Z"},"id":"BWyEX5eCj1by","outputId":"68438e1b-6c88-4289-b3a4-0b5707c14663"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Dataset path and Transforms**","metadata":{}},{"cell_type":"code","source":"# 2. Paths & Transforms\n\ndata_dir = \"/kaggle/input/ct-kidney-dataset-normal-cyst-tumor-and-stone/CT-KIDNEY-DATASET-Normal-Cyst-Tumor-Stone/CT-KIDNEY-DATASET-Normal-Cyst-Tumor-Stone\"   # ✏️ CHANGE if needed\n\n\nbase_tf   = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406],\n                         [0.229, 0.224, 0.225])\n])\nweak_tf   = transforms.Compose([\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomCrop(224, padding=4),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406],\n                         [0.229, 0.224, 0.225])\n])\nstrong_tf = transforms.Compose([\n    transforms.RandomVerticalFlip(),\n    transforms.RandAugment(7, 15),\n    transforms.RandomCrop(224, padding=4),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406],\n                         [0.229, 0.224, 0.225])\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T20:18:24.816584Z","iopub.execute_input":"2025-07-22T20:18:24.816867Z","iopub.status.idle":"2025-07-22T20:18:24.822942Z","shell.execute_reply.started":"2025-07-22T20:18:24.81684Z","shell.execute_reply":"2025-07-22T20:18:24.822224Z"},"id":"JFIY6mroj1b1"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Dataset Split**","metadata":{}},{"cell_type":"code","source":"# 3 · ImageFolder (NO transform yet) & splits\n\nds_raw = datasets.ImageFolder(root=data_dir)  # ← PIL images\nclass_names = ds_raw.classes\nnum_classes = len(class_names)\nprint(f\"Total images: {len(ds_raw)} | Classes: {class_names}\")\n\n# 80% for training\n# 10% for testing\n# 10% for validation\n\nlen_full  = len(ds_raw)\ntest_sz   = len_full // 10\nval_sz    = len_full // 10\ntrain_sz  = len_full - val_sz - test_sz\ntrain_ds, val_ds, test_ds = random_split(\n    ds_raw, [train_sz, val_sz, test_sz],\n    generator=torch.Generator().manual_seed(SEED)\n)\nprint(f\"Train {train_sz} | Val {val_sz} | Test {test_sz}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T20:18:24.824542Z","iopub.execute_input":"2025-07-22T20:18:24.824775Z","iopub.status.idle":"2025-07-22T20:18:35.052604Z","shell.execute_reply.started":"2025-07-22T20:18:24.824759Z","shell.execute_reply":"2025-07-22T20:18:35.051934Z"},"id":"bLmrFslMj1b2","outputId":"e955fe5c-8191-4a60-831a-5cc378269c79"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Labeled & Unlabeled**","metadata":{}},{"cell_type":"code","source":"# 4 · Labeled vs Unlabeled split inside TRAIN\n\nlabel_frac = 0.10             # 10 % of train data has labels\ntrain_indices = list(range(train_sz))\nrandom.shuffle(train_indices)\nn_lab = int(label_frac * train_sz)\nlab_idx, unlab_idx = train_indices[:n_lab], train_indices[n_lab:]\n\nlab_ds   = Subset(train_ds, lab_idx)\nunlab_ds = Subset(train_ds, unlab_idx)\nprint(f\"Labeled {len(lab_ds)} | Unlabeled {len(unlab_ds)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T20:18:35.053419Z","iopub.execute_input":"2025-07-22T20:18:35.053635Z","iopub.status.idle":"2025-07-22T20:18:35.061642Z","shell.execute_reply.started":"2025-07-22T20:18:35.053618Z","shell.execute_reply":"2025-07-22T20:18:35.060929Z"},"id":"FODXxzNtj1b3","outputId":"3acc704f-9f67-4b11-8424-421860be6cf8"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**FixMatch Function**","metadata":{}},{"cell_type":"code","source":"# 5 · Dataset wrappers\n\nclass FixMatchDataset(Dataset):\n    def __init__(self, subset, labelled=True):\n        self.subset  = subset\n        self.labelled = labelled\n    def __len__(self): return len(self.subset)\n    def __getitem__(self, i):\n        img, y = self.subset[i]  # PIL image\n        if self.labelled:\n            return weak_tf(img), y\n        return weak_tf(img), strong_tf(img)\n\nclass EvalDataset(Dataset):\n    def __init__(self, subset): self.subset = subset\n    def __len__(self): return len(self.subset)\n    def __getitem__(self, i):\n        img, y = self.subset[i]\n        return base_tf(img), y\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T20:18:35.062369Z","iopub.execute_input":"2025-07-22T20:18:35.062581Z","iopub.status.idle":"2025-07-22T20:18:35.091719Z","shell.execute_reply.started":"2025-07-22T20:18:35.062564Z","shell.execute_reply":"2025-07-22T20:18:35.091019Z"},"id":"hvK5w4zGj1b5"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Load Dataset**","metadata":{}},{"cell_type":"code","source":"# 6 · DataLoaders\n\nbatch_lab, batch_unlab = 32, 64\nlab_loader   = DataLoader(FixMatchDataset(lab_ds,  True),\n                          batch_size=batch_lab, shuffle=True,\n                          drop_last=True, num_workers=0)\nunlab_loader = DataLoader(FixMatchDataset(unlab_ds, False),\n                          batch_size=batch_unlab, shuffle=True,\n                          drop_last=True, num_workers=0)\nval_loader   = DataLoader(EvalDataset(val_ds),\n                          batch_size=64, shuffle=False, num_workers=0)\ntest_loader  = DataLoader(EvalDataset(test_ds),\n                          batch_size=64, shuffle=False, num_workers=0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T20:18:35.092429Z","iopub.execute_input":"2025-07-22T20:18:35.092719Z","iopub.status.idle":"2025-07-22T20:18:35.106631Z","shell.execute_reply.started":"2025-07-22T20:18:35.092696Z","shell.execute_reply":"2025-07-22T20:18:35.105942Z"},"id":"xKOA6IsAj1b6"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Swin transformer, Frozen BackBone, Model, hyperr-parameters**","metadata":{}},{"cell_type":"code","source":"# 7 · Swin Transformer (Frozen Backbone) with Linear Probe\n\n# Load Swin Transformer without classification head\nbackbone = timm.create_model('swin_tiny_patch4_window7_224', pretrained=True, num_classes=0)\n\n# Freeze all backbone layers\nfor param in backbone.parameters():\n    param.requires_grad = False\n\n# Define a linear probe on top of the frozen backbone\nclass SwinWithLinearHead(nn.Module):\n    def __init__(self, backbone, num_classes):\n        super().__init__()\n        self.backbone = backbone\n        self.head = nn.Linear(backbone.num_features, num_classes)\n\n    def forward(self, x):\n        x = self.backbone(x)\n        x = self.head(x)\n        return x\n\n# Set number of classes\nnum_classes = 4  # Change this to match your dataset\n\n# Create the model\nmodel = SwinWithLinearHead(backbone, num_classes).to(device)\n\n# Only train the linear head\noptimizer = optim.AdamW(model.head.parameters(), lr=3e-4, weight_decay=1e-4)\n\n# Hyperparameters\nepochs   = 300\ntau      = 0.95     # confidence threshold\nlambda_u = 1.0      # unsupervised loss weight\n\n# For tracking loss\nsup_hist, unsup_hist, val_hist = [], [], []\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T20:18:35.107189Z","iopub.execute_input":"2025-07-22T20:18:35.107409Z","iopub.status.idle":"2025-07-22T20:18:40.183241Z","shell.execute_reply.started":"2025-07-22T20:18:35.107377Z","shell.execute_reply":"2025-07-22T20:18:40.182438Z"},"id":"Dk31-uWLj1b6","outputId":"4693961a-ecc1-4686-a43b-0571b3486a7c"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Training Loop**","metadata":{}},{"cell_type":"code","source":"# 8 · Training loop  (FixMatch + tqdm)\n\n\nfor epoch in trange(1, epochs + 1, desc=\"Epoch\"):\n    model.train()\n    sup_meter = unsup_meter = 0.0\n\n     # --- training progress bar ---\n    train_bar = tqdm(\n        zip(lab_loader, unlab_loader),\n        total=min(len(lab_loader), len(unlab_loader)),\n        desc=f\"Train {epoch:02d}\", leave=False\n    )\n\n    for (x_lab, y_lab), (w_unlab, s_unlab) in train_bar:\n        x_lab, y_lab     = x_lab.to(device), y_lab.to(device)\n        w_unlab, s_unlab = w_unlab.to(device), s_unlab.to(device)\n\n        # --- supervised loss ---\n        logits_lab = model(x_lab)\n        loss_sup   = F.cross_entropy(logits_lab, y_lab)\n\n        # ---  generate pseudo-label on weak aug ---\n        with torch.no_grad():\n            logits_w = model(w_unlab)\n            probs_w  = F.softmax(logits_w, dim=1)\n            max_p, pseudo = probs_w.max(1)\n            mask = (max_p >= tau).float()  # 1 if confident\n\n        # --- unsupervised loss ---\n        logits_s = model(s_unlab)\n        loss_uns = (F.cross_entropy(logits_s, pseudo,\n                                    reduction=\"none\") * mask).mean()\n\n        # --- total loss and optimization ---\n        loss = loss_sup + lambda_u * loss_uns\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        sup_meter  += loss_sup.item()\n        unsup_meter += loss_uns.item()\n\n        train_bar.set_postfix(Sup=f\"{loss_sup.item():.4f}\",\n                              Unsup=f\"{loss_uns.item():.4f}\")\n\n    # --- record average training losses --\n    \n    sup_epoch   = sup_meter  / len(lab_loader)\n    unsup_epoch = unsup_meter/ len(unlab_loader)\n    sup_hist.append(sup_epoch)\n    unsup_hist.append(unsup_epoch)\n\n    # ---------- validation ----------\n    model.eval()\n    val_loss = correct = total = 0\n    with torch.no_grad():\n        for x_val, y_val in val_loader:\n            x_val, y_val = x_val.to(device), y_val.to(device)\n            logits = model(x_val)\n            val_loss += F.cross_entropy(logits, y_val,\n                                        reduction=\"sum\").item()\n            preds = logits.argmax(1)\n            correct += (preds == y_val).sum().item()\n            total   += y_val.size(0)\n    \n     # --- record validation loss and accuracy ---\n    val_epoch = val_loss / total\n    val_hist.append(val_epoch)\n    val_acc   = correct / total\n\n    tqdm.write(f\"Epoch {epoch:02d} | \"\n               f\"Sup {sup_epoch:.4f} | Unsup {unsup_epoch:.4f} | \"\n               f\"ValLoss {val_epoch:.4f} | ValAcc {val_acc:.3%}\")\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T20:18:40.184117Z","iopub.execute_input":"2025-07-22T20:18:40.184342Z","iopub.status.idle":"2025-07-23T03:36:11.333996Z","shell.execute_reply.started":"2025-07-22T20:18:40.184323Z","shell.execute_reply":"2025-07-23T03:36:11.333177Z"},"id":"1JGFARuAj1b7","outputId":"8692ff01-ec3a-420a-ffa2-e33b02b407a8"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Loss Curve**","metadata":{}},{"cell_type":"code","source":"# 9 · Training vs Validation Loss Curves\n\nimport matplotlib.pyplot as plt\n\nepochs_range = range(1, len(sup_hist) + 1)\ntrain_loss = [s + lambda_u * u for s, u in zip(sup_hist, unsup_hist)]\n\nplt.figure(figsize=(8, 5))\nplt.plot(epochs_range, train_loss, label=\"Training Loss\", marker='o')\nplt.plot(epochs_range, val_hist, label=\"Validation Loss\", marker='x')\n\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.title(\"FixMatch – Swin Transformer (Frozen Backbone + Linear Probe)\")\nplt.legend()\nplt.grid(True, linestyle=\"--\", linewidth=0.5)\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T03:36:11.341846Z","iopub.execute_input":"2025-07-23T03:36:11.342042Z","iopub.status.idle":"2025-07-23T03:36:11.694516Z","shell.execute_reply.started":"2025-07-23T03:36:11.342011Z","shell.execute_reply":"2025-07-23T03:36:11.693806Z"},"id":"VKGIW5mhx9M9","outputId":"5d03e449-6925-4ae2-ddbd-674e29811b33"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Checkpoint**","metadata":{}},{"cell_type":"code","source":"# 10 · Best Validation Epoch\n\nbest_epoch = val_hist.index(min(val_hist)) + 1\nbest_val_loss = min(val_hist)\n\nprint(f\"\\n✅ Best Validation Epoch: {best_epoch}\")\nprint(f\"📉 Best Validation Loss: {best_val_loss:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T03:36:11.695237Z","iopub.execute_input":"2025-07-23T03:36:11.695459Z","iopub.status.idle":"2025-07-23T03:36:11.699852Z","shell.execute_reply.started":"2025-07-23T03:36:11.695434Z","shell.execute_reply":"2025-07-23T03:36:11.699147Z"},"id":"DnYXxMk6x9M9","outputId":"32979672-8a7b-443f-a461-b3db8581a7e0"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Accuracy Report**","metadata":{}},{"cell_type":"code","source":"# 11 · Test-set evaluation\n\nmodel.eval();\ntotal = correct = 0\nall_preds, all_lbls = [], []\nwith torch.no_grad():\n    for x_test, y_test in tqdm(test_loader, desc=\"Testing\", leave=False):\n        x_test, y_test = x_test.to(device), y_test.to(device)\n        logits = model(x_test)\n        preds  = logits.argmax(1)\n        correct += (preds == y_test).sum().item()\n        total   += y_test.size(0)\n        all_preds.extend(preds.cpu().numpy())\n        all_lbls.extend(y_test.cpu().numpy())\n\ntest_acc = correct / total\nprint(f\"\\nTest accuracy: {test_acc:.3%}\\n\")\nprint(classification_report(all_lbls, all_preds,\n                            target_names=class_names, digits=3))\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T03:36:11.700614Z","iopub.execute_input":"2025-07-23T03:36:11.700777Z","iopub.status.idle":"2025-07-23T03:36:32.408817Z","shell.execute_reply.started":"2025-07-23T03:36:11.700764Z","shell.execute_reply":"2025-07-23T03:36:32.408232Z"},"id":"4JU2RFKHj1b9","outputId":"85caa11e-a9d1-42c7-a5c5-242c1fd90f15"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Save the Model**","metadata":{}},{"cell_type":"code","source":"# 12 · Save the model \n\ntorch.save(model.state_dict(), \"fixmatch_swin_kidney.pth\")\nprint(\"Model saved to fixmatch_swin_kidney.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T03:36:32.409582Z","iopub.execute_input":"2025-07-23T03:36:32.409809Z","iopub.status.idle":"2025-07-23T03:36:32.567143Z","shell.execute_reply.started":"2025-07-23T03:36:32.409791Z","shell.execute_reply":"2025-07-23T03:36:32.566331Z"},"id":"ZV2Mvbvhj1b9","outputId":"45e24c18-cf6a-4ccb-fafd-b8066207c21e"},"outputs":[],"execution_count":null}]}